{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f93df15-7531-4548-b1c7-f41ca53452b1",
   "metadata": {},
   "source": [
    "# Autism Dataset w/ Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85895e0d-f849-4e1e-9be7-f646ea839890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U ucimlrepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da40d270-aae3-4ca8-ae71-e937419fb6e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error connecting to server",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m     h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1349\u001b[0m               encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1294\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1340\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1339\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1466\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1464\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m-> 1466\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mwrap_socket(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock,\n\u001b[0;32m   1467\u001b[0m                                       server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    524\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1383\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1382\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ucimlrepo\\fetch.py:68\u001b[0m, in \u001b[0;36mfetch_ucirepo\u001b[1;34m(name, id)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     response \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(api_url, context\u001b[38;5;241m=\u001b[39mssl\u001b[38;5;241m.\u001b[39mcreate_default_context(cafile\u001b[38;5;241m=\u001b[39mcertifi\u001b[38;5;241m.\u001b[39mwhere()))\n\u001b[0;32m     69\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    537\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    495\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPSConnection, req,\n\u001b[0;32m   1392\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context, check_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_hostname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1352\u001b[0m r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# fetch dataset \u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m autism_screening_adult \u001b[38;5;241m=\u001b[39m fetch_ucirepo(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m426\u001b[39m) \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# data (as pandas dataframes) \u001b[39;00m\n\u001b[0;32m     13\u001b[0m X \u001b[38;5;241m=\u001b[39m autism_screening_adult\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfeatures \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ucimlrepo\\fetch.py:71\u001b[0m, in \u001b[0;36mfetch_ucirepo\u001b[1;34m(name, id)\u001b[0m\n\u001b[0;32m     69\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(response)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError):\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError connecting to server\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# verify that dataset exists \u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[1;31mConnectionError\u001b[0m: Error connecting to server"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# fetch dataset \n",
    "autism_screening_adult = fetch_ucirepo(id=426) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = autism_screening_adult.data.features \n",
    "y = autism_screening_adult.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ae326f2-152c-43f3-bf62-a44b58b2f45f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "print(X.head())\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8e0ab65-45ae-48e2-b8f7-08c4fed24d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124methnicity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mused_app_before\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdropna(subset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mselect_dtypes(include \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "X = X.drop(['ethnicity', 'relation', 'used_app_before'], axis=1)\n",
    "X = X.dropna(subset = ['age'])\n",
    "for col in X.select_dtypes(include = ['object']).columns:\n",
    "    X[col] = pd.factorize(X[col])[0]\n",
    "X = X.drop_duplicates()\n",
    "X['age'] = X['age'].astype(int)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6684c17-a05d-4b97-bf65-a8acd3a41cbd",
   "metadata": {},
   "source": [
    "#### Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95a4e7-94db-434f-b6db-b6fc22249374",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_and_Y = pd.concat([X,y], axis = 1).to_numpy()\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(X_and_Y)\n",
    "X = X_and_Y[:, :-1]\n",
    "y = X_and_Y[:, -1]\n",
    "y[y==0] = -1\n",
    "total_samples = X.shape[0]\n",
    "print(total_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5970e36-4d6a-4fd3-9747-7daf814bd0ad",
   "metadata": {},
   "source": [
    "### Classifier #1: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78fcb5d9-8e7d-4da9-b9d3-14149836853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracies for Each Hyperparameter Combination:\n",
      "==================================================\n",
      "Parameters: {'max_depth': 5, 'min_samples_split': 2}, Validation Accuracy: 0.9064\n",
      "Parameters: {'max_depth': 5, 'min_samples_split': 10}, Validation Accuracy: 0.9957\n",
      "Parameters: {'max_depth': 5, 'min_samples_split': 20}, Validation Accuracy: 0.9957\n",
      "Parameters: {'max_depth': 10, 'min_samples_split': 2}, Validation Accuracy: 0.9064\n",
      "Parameters: {'max_depth': 10, 'min_samples_split': 10}, Validation Accuracy: 0.9957\n",
      "Parameters: {'max_depth': 10, 'min_samples_split': 20}, Validation Accuracy: 0.9957\n",
      "Parameters: {'max_depth': None, 'min_samples_split': 2}, Validation Accuracy: 0.9064\n",
      "Parameters: {'max_depth': None, 'min_samples_split': 10}, Validation Accuracy: 0.9957\n",
      "Parameters: {'max_depth': None, 'min_samples_split': 20}, Validation Accuracy: 0.9957\n",
      "\n",
      "Best Hyperparameters:\n",
      "  {'max_depth': 5, 'min_samples_split': 10}\n",
      "Validation Accuracy with Best Hyperparameters: 0.9957\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters - need to use cross validation and compute validation accuracy\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 10, 20]\n",
    "}\n",
    "\n",
    "# Hyperparamter tuning in order to get the best hyperparameters - perform a 3-fold cross-validation\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy', return_train_score=True)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Extract and display the validation accuracy for each combination\n",
    "print(\"Validation Accuracies for Each Hyperparameter Combination:\")\n",
    "print(\"=\" * 50)\n",
    "cv_results = grid_search.cv_results_\n",
    "for mean_val_acc, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "    print(f\"Parameters: {params}, Validation Accuracy: {mean_val_acc:.4f}\")\n",
    "\n",
    "# Best hyperparameters based on validation accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_validation_accuracy = grid_search.best_score_\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"  {best_params}\")\n",
    "print(f\"Validation Accuracy with Best Hyperparameters: {best_validation_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13f405-6643-4e53-8774-065602814894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Partition splits with 80/20, 50/50, 20/80\n",
    "splits = [(.8, .2), (.5, .5), (.2, .8)]\n",
    "results = {}\n",
    "split_results = []  # Store split-wise metrics\n",
    "\n",
    "# Outer loop for each partition\n",
    "for split in splits:\n",
    "    train_size, test_size = split\n",
    "    trial_accuracies = {'train': [], 'test': []}\n",
    "    trial_precisions = []\n",
    "    trial_recalls = []\n",
    "    trial_f1_scores = []\n",
    "\n",
    "    # Inner loop to run 3 trials per split\n",
    "    for trial in range(3): \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=None)\n",
    "\n",
    "        # Retrain the model with the best hyperparameters\n",
    "        best_clf = DecisionTreeClassifier(\n",
    "            max_depth=best_params['max_depth'], \n",
    "            min_samples_split=best_params['min_samples_split'], \n",
    "            random_state=42\n",
    "        )\n",
    "        best_clf.fit(X_train, y_train)\n",
    "\n",
    "        # Compute train accuracy\n",
    "        y_train_pred = best_clf.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        trial_accuracies['train'].append(train_accuracy)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        y_test_pred = best_clf.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        trial_accuracies['test'].append(test_accuracy)\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        trial_precisions.append(precision_score(y_test, y_test_pred, average=\"weighted\", zero_division=0))\n",
    "        trial_recalls.append(recall_score(y_test, y_test_pred, average=\"weighted\"))\n",
    "        trial_f1_scores.append(f1_score(y_test, y_test_pred, average=\"weighted\"))\n",
    "\n",
    "    # Average metrics over the 3 trials for the current split\n",
    "    avg_train_accuracy = np.mean(trial_accuracies['train'])\n",
    "    avg_test_accuracy = np.mean(trial_accuracies['test'])\n",
    "    avg_precision = np.mean(trial_precisions)\n",
    "    avg_recall = np.mean(trial_recalls)\n",
    "    avg_f1_score = np.mean(trial_f1_scores)\n",
    "\n",
    "    # Save split-level metrics\n",
    "    split_results.append({\n",
    "        'Split': f\"{int(train_size * 100)}/{int(test_size * 100)}\",\n",
    "        'Test Accuracy': avg_test_accuracy,\n",
    "        'Precision': avg_precision,\n",
    "        'Recall': avg_recall,\n",
    "        'F1-Score': avg_f1_score\n",
    "    })\n",
    "\n",
    "    # Print split-specific results\n",
    "    print(f\"Results for {int(train_size * 100)}/{int(test_size * 100)} Split:\")\n",
    "    print(f\"Train Accuracy: {avg_train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {avg_test_accuracy:.4f}\")\n",
    "    # print(f\"Precision: {avg_precision:.4f}\")\n",
    "    # print(f\"Recall: {avg_recall:.4f}\")\n",
    "    # print(f\"F1-Score: {avg_f1_score:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# # Save the metrics to use later for the summary table\n",
    "# print(\"Overall Metrics for Decision Tree Classifier (Autism Dataset):\")\n",
    "# print(f\"Overall Accuracy: {overall_accuracy_1:.4f}\")\n",
    "# print(f\"Overall Precision: {overall_precision_1:.4f}\")\n",
    "# print(f\"Overall Recall: {overall_recall_1:.4f}\")\n",
    "# print(f\"Overall F1-Score: {overall_f1_score_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0603c-9696-4df2-9a51-59d006965c34",
   "metadata": {},
   "source": [
    "#### Results for Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c19d0-9185-4a8a-a3e5-892f007e3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics for the Decision Tree classifier\n",
    "overall_accuracy_1 = np.mean([r['Test Accuracy'] for r in split_results])\n",
    "overall_precision_1 = np.mean([r['Precision'] for r in split_results])\n",
    "overall_recall_1 = np.mean([r['Recall'] for r in split_results])\n",
    "overall_f1_score_1 = np.mean([r['F1-Score'] for r in split_results])\n",
    "# Create a formatted table for the split results\n",
    "print(\"Decision Tree Classifier Results on Autism Dataset:\")\n",
    "print(f\"{'Split':<10}{'ACC':<15}{'Precision':<15}{'Recall':<15}{'F1-Score':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Loop through the split results and display each row\n",
    "for result in split_results:\n",
    "    print(f\"{result['Split']:<10}{result['Test Accuracy']:<15.4f}{result['Precision']:<15.4f}{result['Recall']:<15.4f}{result['F1-Score']:<15.4f}\")\n",
    "\n",
    "# Print the overall metrics as the last row\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Overall':<10}{overall_accuracy_1:<15.4f}{overall_precision_1:<15.4f}{overall_recall_1:<15.4f}{overall_f1_score_1:<15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef5eb7-4de2-4174-a051-8e3db4f583f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a76438e-c582-47e3-9eea-1ec51c61c48e",
   "metadata": {},
   "source": [
    "### Classifier #2: SVM (with RBF kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16a0903-ecd7-429c-a228-7008c39c8af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracies for Each Hyperparameter Combination:\n",
      "Parameters: {'C': 1, 'gamma': 1e-06, 'kernel': 'rbf'}, Validation Accuracy: 0.7315\n",
      "Parameters: {'C': 1, 'gamma': 1e-05, 'kernel': 'rbf'}, Validation Accuracy: 0.7315\n",
      "Parameters: {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}, Validation Accuracy: 0.7315\n",
      "Parameters: {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}, Validation Accuracy: 0.9346\n",
      "Parameters: {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}, Validation Accuracy: 0.9460\n",
      "Parameters: {'C': 10, 'gamma': 1e-06, 'kernel': 'rbf'}, Validation Accuracy: 0.7315\n",
      "Parameters: {'C': 10, 'gamma': 1e-05, 'kernel': 'rbf'}, Validation Accuracy: 0.7315\n",
      "Parameters: {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}, Validation Accuracy: 0.9659\n",
      "Parameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}, Validation Accuracy: 0.9773\n",
      "Parameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}, Validation Accuracy: 0.9673\n",
      "Parameters: {'C': 100, 'gamma': 1e-06, 'kernel': 'rbf'}, Validation Accuracy: 0.7315\n",
      "Parameters: {'C': 100, 'gamma': 1e-05, 'kernel': 'rbf'}, Validation Accuracy: 0.9616\n",
      "Parameters: {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}, Validation Accuracy: 0.9801\n",
      "Parameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}, Validation Accuracy: 0.9872\n",
      "Parameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}, Validation Accuracy: 0.9659\n",
      "Parameters: {'C': 1000, 'gamma': 1e-06, 'kernel': 'rbf'}, Validation Accuracy: 0.9588\n",
      "Parameters: {'C': 1000, 'gamma': 1e-05, 'kernel': 'rbf'}, Validation Accuracy: 0.9787\n",
      "Parameters: {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}, Validation Accuracy: 0.9943\n",
      "Parameters: {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}, Validation Accuracy: 0.9858\n",
      "Parameters: {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}, Validation Accuracy: 0.9659\n",
      "Parameters: {'C': 10000, 'gamma': 1e-06, 'kernel': 'rbf'}, Validation Accuracy: 0.9829\n",
      "Parameters: {'C': 10000, 'gamma': 1e-05, 'kernel': 'rbf'}, Validation Accuracy: 0.9957\n",
      "Parameters: {'C': 10000, 'gamma': 0.0001, 'kernel': 'rbf'}, Validation Accuracy: 0.9943\n",
      "Parameters: {'C': 10000, 'gamma': 0.001, 'kernel': 'rbf'}, Validation Accuracy: 0.9716\n",
      "Parameters: {'C': 10000, 'gamma': 0.01, 'kernel': 'rbf'}, Validation Accuracy: 0.9659\n",
      "\n",
      "Best Hyperparameters:\n",
      "  {'C': 10000, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "Validation Accuracy with Best Hyperparameters: 0.9957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Define the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100, 1000, 10000],\n",
    "    'gamma': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Set up the SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Perform GridSearchCV on the entire dataset\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=3, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_imputed, y)\n",
    "\n",
    "# Extract and print the results\n",
    "cv_results = grid_search.cv_results_\n",
    "print(\"Validation Accuracies for Each Hyperparameter Combination:\")\n",
    "for mean_val_acc, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "    print(f\"Parameters: {params}, Validation Accuracy: {mean_val_acc:.4f}\")\n",
    "\n",
    "# Best hyperparameters based on validation accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_validation_accuracy = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding validation accuracy\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"  {best_params}\")\n",
    "print(f\"Validation Accuracy with Best Hyperparameters: {best_validation_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4bbc2ee-dfb5-4ba8-95c6-68cb11e9037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for 80/20 Split:\n",
      "Train Accuracy: 0.9953\n",
      "Test Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for 50/50 Split:\n",
      "Train Accuracy: 0.9953\n",
      "Test Accuracy: 0.9943\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for 20/80 Split:\n",
      "Train Accuracy: 0.9929\n",
      "Test Accuracy: 0.9823\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Split ratios\n",
    "split_ratios = [(0.8, 0.2), (0.5, 0.5), (0.2, 0.8)]\n",
    "split_results_svm = []\n",
    "\n",
    "# Best hyperparameters for the SVM\n",
    "best_C = 10000\n",
    "best_gamma = 1e-05\n",
    "\n",
    "# Iterate over splits\n",
    "for train_size, test_size in split_ratios:\n",
    "    trial_train_accuracies = []\n",
    "    trial_validation_accuracies = []\n",
    "    trial_test_accuracies = []\n",
    "    trial_precisions = []\n",
    "    trial_recalls = []\n",
    "    trial_f1_scores = []\n",
    "    \n",
    "    for _ in range(3):  # Perform 3 trials for each split\n",
    "        # Split the data into train and temp (validation + test)\n",
    "        X_train, X_temp, Y_train, Y_temp = train_test_split(X, y, train_size=train_size, random_state=None)\n",
    "        \n",
    "        # Now split the temp data into validation and test\n",
    "        X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=test_size, random_state=None)\n",
    "        \n",
    "        # Impute missing values on the training data\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_train_imputed = imputer.fit_transform(X_train)  # Fit and transform the training data\n",
    "        X_val_imputed = imputer.transform(X_val)  # Transform the validation data\n",
    "        X_test_imputed = imputer.transform(X_test)  # Transform the test data\n",
    "\n",
    "        # Train the SVM model using the best hyperparameters\n",
    "        clf = SVC(C=best_C, gamma=best_gamma, kernel='rbf')\n",
    "        clf.fit(X_train_imputed, Y_train)\n",
    "\n",
    "        # Calculate train accuracy\n",
    "        train_accuracy = clf.score(X_train_imputed, Y_train)\n",
    "        trial_train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        test_accuracy = clf.score(X_test_imputed, Y_test)\n",
    "        trial_test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        Y_test_pred = clf.predict(X_test_imputed)\n",
    "        trial_precisions.append(precision_score(Y_test, Y_test_pred, average=\"weighted\", zero_division=0))\n",
    "        trial_recalls.append(recall_score(Y_test, Y_test_pred, average=\"weighted\"))\n",
    "        trial_f1_scores.append(f1_score(Y_test, Y_test_pred, average=\"weighted\"))\n",
    "\n",
    "    # Calculate and store split-wise average metrics\n",
    "    avg_train_accuracy = np.mean(trial_train_accuracies)\n",
    "    avg_test_accuracy = np.mean(trial_test_accuracies)\n",
    "    avg_precision = np.mean(trial_precisions)\n",
    "    avg_recall = np.mean(trial_recalls)\n",
    "    avg_f1_score = np.mean(trial_f1_scores)\n",
    "\n",
    "    split_results_svm.append({\n",
    "        'Split': f\"{int(train_size * 100)}/{int(test_size * 100)}\",\n",
    "        'Test Accuracy': avg_test_accuracy,\n",
    "        'Precision': avg_precision,\n",
    "        'Recall': avg_recall,\n",
    "        'F1-Score': avg_f1_score\n",
    "    })\n",
    "\n",
    "    # Print split-specific results\n",
    "    print(f\"\\nResults for {int(train_size * 100)}/{int(test_size * 100)} Split:\")\n",
    "    print(f\"Train Accuracy: {avg_train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {avg_test_accuracy:.4f}\")\n",
    "    # print(f\"Precision: {avg_precision:.4f}\")\n",
    "    # print(f\"Recall: {avg_recall:.4f}\")\n",
    "    # print(f\"F1-Score: {avg_f1_score:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72176f6-de10-4096-a247-54b897267cf5",
   "metadata": {},
   "source": [
    "#### Results for SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c25d36e7-2c49-4702-9262-b8ab53ecc6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Results on Autism Dataset:\n",
      "Split     ACC            Precision      Recall         F1-Score       \n",
      "-----------------------------------------------------------------\n",
      "80/20     1.0000         1.0000         1.0000         1.0000         \n",
      "50/50     0.9943         0.9944         0.9943         0.9943         \n",
      "20/80     0.9823         0.9829         0.9823         0.9824         \n",
      "-----------------------------------------------------------------\n",
      "Overall   0.9922         0.9924         0.9922         0.9922         \n"
     ]
    }
   ],
   "source": [
    "# Calculate overall metrics for the SVM classifier\n",
    "overall_accuracy_2 = np.mean([r['Test Accuracy'] for r in split_results_svm])\n",
    "overall_precision_2 = np.mean([r['Precision'] for r in split_results_svm])\n",
    "overall_recall_2 = np.mean([r['Recall'] for r in split_results_svm])\n",
    "overall_f1_score_2 = np.mean([r['F1-Score'] for r in split_results_svm])\n",
    "\n",
    "# Create a formatted table for the split results\n",
    "print(\"SVM Classifier Results on Autism Dataset:\")\n",
    "print(f\"{'Split':<10}{'ACC':<15}{'Precision':<15}{'Recall':<15}{'F1-Score':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Loop through the split results and display each row\n",
    "for result in split_results_svm:\n",
    "    print(f\"{result['Split']:<10}{result['Test Accuracy']:<15.4f}{result['Precision']:<15.4f}{result['Recall']:<15.4f}{result['F1-Score']:<15.4f}\")\n",
    "\n",
    "# Print the overall metrics as the last row\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Overall':<10}{overall_accuracy_2:<15.4f}{overall_precision_2:<15.4f}{overall_recall_2:<15.4f}{overall_f1_score_2:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42903493-d53f-4883-9b6d-f8647345720e",
   "metadata": {},
   "source": [
    "### Classifier #3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d4cdd6f-ea17-4021-b1e7-8a418aa34a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracies for Each Hyperparameter Combination:\n",
      "Parameters: {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}, Validation Accuracy: 0.8267\n",
      "Parameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}, Validation Accuracy: 0.8495\n",
      "Parameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}, Validation Accuracy: 0.9503\n",
      "Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}, Validation Accuracy: 0.8992\n",
      "Parameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}, Validation Accuracy: 0.9957\n",
      "Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}, Validation Accuracy: 0.9446\n",
      "Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}, Validation Accuracy: 0.9957\n",
      "Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}, Validation Accuracy: 0.9829\n",
      "Parameters: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}, Validation Accuracy: 0.9957\n",
      "Parameters: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}, Validation Accuracy: 0.9957\n",
      "\n",
      "Best Hyperparameters:\n",
      "  {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Validation Accuracy with Best Hyperparameters: 0.9957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],  # Regularization types\n",
    "    'solver': ['liblinear']   # Compatible solver for l1 and l2 penalties\n",
    "}\n",
    "\n",
    "# Set up the Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Perform GridSearchCV on the entire dataset\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=3, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_imputed, y)\n",
    "\n",
    "# Extract and print the results\n",
    "cv_results = grid_search.cv_results_\n",
    "print(\"Validation Accuracies for Each Hyperparameter Combination:\")\n",
    "for mean_val_acc, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "    print(f\"Parameters: {params}, Validation Accuracy: {mean_val_acc:.4f}\")\n",
    "\n",
    "# Best hyperparameters based on validation accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_validation_accuracy = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding validation accuracy\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"  {best_params}\")\n",
    "print(f\"Validation Accuracy with Best Hyperparameters: {best_validation_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16942c5d-04bc-4e69-8ded-e61d008434a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for 80/20 Split:\n",
      "Train Accuracy: 0.9953\n",
      "Test Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for 50/50 Split:\n",
      "Train Accuracy: 0.9953\n",
      "Test Accuracy: 0.9962\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for 20/80 Split:\n",
      "Train Accuracy: 0.9929\n",
      "Test Accuracy: 0.9720\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the best hyperparameters from previous tuning\n",
    "best_C = 1\n",
    "best_penalty = 'l1'\n",
    "best_solver = 'liblinear'\n",
    "\n",
    "# Split ratios\n",
    "split_ratios = [(0.8, 0.2), (0.5, 0.5), (0.2, 0.8)]\n",
    "split_results_logreg = []\n",
    "\n",
    "# Lists for storing accuracies and metrics\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "test_accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over splits\n",
    "for train_size, test_size in split_ratios:\n",
    "    trial_train_accuracies = []\n",
    "    trial_validation_accuracies = []\n",
    "    trial_test_accuracies = []\n",
    "    trial_precisions = []\n",
    "    trial_recalls = []\n",
    "    trial_f1_scores = []\n",
    "    \n",
    "    for _ in range(3):  # Perform 3 trials for each split\n",
    "        # Split the data into train and temp (validation + test)\n",
    "        X_train, X_temp, Y_train, Y_temp = train_test_split(X, y, train_size=train_size, random_state=None)\n",
    "        \n",
    "        # Now split the temp data into validation and test\n",
    "        X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=test_size, random_state=None)\n",
    "        \n",
    "        # Impute missing values on the training data\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_train_imputed = imputer.fit_transform(X_train)  # Fit and transform the training data\n",
    "        X_val_imputed = imputer.transform(X_val)  # Transform the validation data\n",
    "        X_test_imputed = imputer.transform(X_test)  # Transform the test data\n",
    "\n",
    "        # Train the Logistic Regression model using the best hyperparameters\n",
    "        clf = LogisticRegression(C=best_C, penalty=best_penalty, solver=best_solver, max_iter=10000)\n",
    "        clf.fit(X_train_imputed, Y_train)\n",
    "\n",
    "        # Calculate train accuracy\n",
    "        train_accuracy = clf.score(X_train_imputed, Y_train)\n",
    "        trial_train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        validation_accuracy = clf.score(X_val_imputed, Y_val)\n",
    "        trial_validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        test_accuracy = clf.score(X_test_imputed, Y_test)\n",
    "        trial_test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        Y_test_pred = clf.predict(X_test_imputed)\n",
    "        trial_precisions.append(precision_score(Y_test, Y_test_pred, average=\"weighted\", zero_division=0))\n",
    "        trial_recalls.append(recall_score(Y_test, Y_test_pred, average=\"weighted\"))\n",
    "        trial_f1_scores.append(f1_score(Y_test, Y_test_pred, average=\"weighted\"))\n",
    "\n",
    "    # Calculate and store split-wise average metrics\n",
    "    avg_train_accuracy = np.mean(trial_train_accuracies)\n",
    "    avg_validation_accuracy = np.mean(trial_validation_accuracies)\n",
    "    avg_test_accuracy = np.mean(trial_test_accuracies)\n",
    "    avg_precision = np.mean(trial_precisions)\n",
    "    avg_recall = np.mean(trial_recalls)\n",
    "    avg_f1_score = np.mean(trial_f1_scores)\n",
    "\n",
    "    split_results_logreg.append({\n",
    "        'Split': f\"{int(train_size * 100)}/{int(test_size * 100)}\",\n",
    "        'Train Accuracy': avg_train_accuracy,\n",
    "        'Validation Accuracy': avg_validation_accuracy,\n",
    "        'Test Accuracy': avg_test_accuracy,\n",
    "        'Precision': avg_precision,\n",
    "        'Recall': avg_recall,\n",
    "        'F1-Score': avg_f1_score\n",
    "    })\n",
    "\n",
    "    # Print split-specific results\n",
    "    print(f\"\\nResults for {int(train_size * 100)}/{int(test_size * 100)} Split:\")\n",
    "    print(f\"Train Accuracy: {avg_train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {avg_test_accuracy:.4f}\")\n",
    "    # print(f\"Precision: {avg_precision:.4f}\")\n",
    "    # print(f\"Recall: {avg_recall:.4f}\")\n",
    "    # print(f\"F1-Score: {avg_f1_score:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d55bd-59b0-4d4f-a350-b4be1b429ea8",
   "metadata": {},
   "source": [
    "#### Results for Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "110f19b5-3c35-44c5-8e11-3d3b861eef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier Results on Autism Dataset:\n",
      "Split     ACC            Precision      Recall         F1-Score       Overall Accuracy 3  \n",
      "--------------------------------------------------------------------------------\n",
      "80/20     1.0000         1.0000         1.0000         1.0000         0.9894              \n",
      "50/50     0.9962         0.9962         0.9962         0.9962         0.9894              \n",
      "20/80     0.9720         0.9722         0.9720         0.9720         0.9894              \n",
      "--------------------------------------------------------------------------------\n",
      "Overall   0.9894         0.9895         0.9894         0.9894         0.9894              \n"
     ]
    }
   ],
   "source": [
    "# Calculate overall metrics for the Logistic Regression classifier\n",
    "overall_accuracy_3 = np.mean([r['Test Accuracy'] for r in split_results_logreg])\n",
    "overall_precision_3 = np.mean([r['Precision'] for r in split_results_logreg])\n",
    "overall_recall_3 = np.mean([r['Recall'] for r in split_results_logreg])\n",
    "overall_f1_score_3 = np.mean([r['F1-Score'] for r in split_results_logreg])\n",
    "\n",
    "# Create a formatted table for the split results\n",
    "print(\"Logistic Regression Classifier Results on Autism Dataset:\")\n",
    "print(f\"{'Split':<10}{'ACC':<15}{'Precision':<15}{'Recall':<15}{'F1-Score':<15}{'Overall Accuracy 3':<20}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Loop through the split results and display each row\n",
    "for result in split_results_logreg:\n",
    "    print(f\"{result['Split']:<10}{result['Test Accuracy']:<15.4f}{result['Precision']:<15.4f}{result['Recall']:<15.4f}{result['F1-Score']:<15.4f}{overall_accuracy_3:<20.4f}\")\n",
    "\n",
    "# Print the overall metrics as the last row\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Overall':<10}{overall_accuracy_3:<15.4f}{overall_precision_3:<15.4f}{overall_recall_3:<15.4f}{overall_f1_score_3:<15.4f}{overall_accuracy_3:<20.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff92ae95-a5b9-4e78-92d7-8bb1a9eaecc9",
   "metadata": {},
   "source": [
    "## Final Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06ce02d8-5041-446f-ae63-721f29f07504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary Table of Overall Metrics:\n",
      "                Classifier       ACC  Precision    Recall  F1-Score\n",
      "0            Decision Tree  0.964850   0.942866  0.964850  0.952067\n",
      "1  Support Vector Machines  0.992206   0.992445  0.992206  0.992239\n",
      "2      Logistic Regression  0.989396   0.989485  0.989396  0.989415\n"
     ]
    }
   ],
   "source": [
    "summary_data = {\n",
    "    'Classifier': ['Decision Tree', 'Support Vector Machines', 'Logistic Regression'],  # Add names for all classifiers\n",
    "    'ACC': [overall_accuracy_1, overall_accuracy_2, overall_accuracy_3],  # Add more as needed\n",
    "    'Precision': [overall_precision_1, overall_precision_2, overall_precision_3],\n",
    "    'Recall': [overall_recall_1, overall_recall_2, overall_recall_3],\n",
    "    'F1-Score': [overall_f1_score_1, overall_f1_score_2, overall_f1_score_3]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Print the summary table\n",
    "print(\"Final Summary Table of Overall Metrics:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfdf4af-eef7-4f43-8302-2c1b34f5e0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
