{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba39fda-a220-418f-b1fa-a98cc014047d",
   "metadata": {},
   "source": [
    "# Liver Dataset w/ Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7b45dea-5565-4648-9fe9-6f85f3f9c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# fetch dataset \n",
    "ilpd_indian_liver_patient_dataset = fetch_ucirepo(id=225) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = ilpd_indian_liver_patient_dataset.data.features \n",
    "y = ilpd_indian_liver_patient_dataset.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b12a1f-1fb6-4bdc-8e6e-cb4200e8b295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Gender    TB   DB  Alkphos  Sgpt  Sgot   TP  ALB  A/G Ratio\n",
      "0   65  Female   0.7  0.1      187    16    18  6.8  3.3       0.90\n",
      "1   62    Male  10.9  5.5      699    64   100  7.5  3.2       0.74\n",
      "2   62    Male   7.3  4.1      490    60    68  7.0  3.3       0.89\n",
      "3   58    Male   1.0  0.4      182    14    20  6.8  3.4       1.00\n",
      "4   72    Male   3.9  2.0      195    27    59  7.3  2.4       0.40\n",
      "Age          0\n",
      "Gender       0\n",
      "TB           0\n",
      "DB           0\n",
      "Alkphos      0\n",
      "Sgpt         0\n",
      "Sgot         0\n",
      "TP           0\n",
      "ALB          0\n",
      "A/G Ratio    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X.head())\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f33d9-b500-400e-915a-c0ee9b419f6e",
   "metadata": {},
   "source": [
    "##### Description of Variables\n",
    "1. Age - Age of the patient\n",
    "2. Gender - Gender of the patient\n",
    "3. TB - Total Bilirubin\n",
    "4. DB - Direct Bilirubin\n",
    "5. Alkphos - Alkaline Phosphotase\n",
    "6. Sgpt - Alamine Aminotransferase\n",
    "7. Sgot - Aspartate Aminotransferase\n",
    "8. TP - Total Proteins\n",
    "9. ALB - Albumin\n",
    "10. A/G Ratio - Albumin and Globulin Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c25d6e6a-65af-4470-b380-672507f98d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(570, 10)\n",
      "(570,)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the 'Gender' variable\n",
    "encoder = LabelEncoder()\n",
    "X.loc[:, 'Gender'] = encoder.fit_transform(X['Gender'])\n",
    "\n",
    "# Fill in missing values for the 'A/G Ratio' column\n",
    "X.loc[:, 'A/G Ratio'] = X['A/G Ratio'].fillna(X['A/G Ratio'].mean())\n",
    "\n",
    "# Reset indices of X and y before dropping duplicates\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "# Drop duplicates in X and align y accordingly\n",
    "X = X.drop_duplicates()\n",
    "y = y.loc[X.index]  # Align y with the updated indices of X\n",
    "\n",
    "# Ensure y is flattened to 1-dimensional\n",
    "y = y.squeeze()  # Convert to a Series if it's multidimensional\n",
    "\n",
    "# Fix: Map labels and convert `y` to numeric type (int)\n",
    "y = y.map({1: 1, 2: -1}).astype(int).to_numpy()  # Adjust mapping and convert to integer array\n",
    "\n",
    "# Convert X to a numpy array\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Check the final shapes of X and y\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88bf96-17b6-4311-b306-19ff82019d73",
   "metadata": {},
   "source": [
    "#### Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed57e38f-f7a4-42e4-8dfd-5f83bea73452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (570, 10)\n",
      "y shape: (570,)\n"
     ]
    }
   ],
   "source": [
    "# Combine X and y into a single array\n",
    "X_and_Y = np.hstack([X, y.reshape(-1, 1)])  # Combine features and labels\n",
    "\n",
    "# Shuffle the combined array\n",
    "np.random.seed(1)  # Set random seed for reproducibility\n",
    "np.random.shuffle(X_and_Y)  # Shuffle rows\n",
    "\n",
    "# Separate X and y after shuffling\n",
    "X = X_and_Y[:, :-1]  # All columns except the last (features)\n",
    "y = X_and_Y[:, -1]   # The last column (target labels)\n",
    "y = y.astype(int)\n",
    "\n",
    "# Check final shapes of X and y\n",
    "print(\"X shape:\", X.shape)  # Features shape\n",
    "print(\"y shape:\", y.shape)  # Labels shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d87ea-71a2-4539-a1d6-19f2123d2952",
   "metadata": {},
   "source": [
    "### Classifier #1: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d78d3d6-ec62-400c-a5b6-8aba1f0ff4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracies for Each Hyperparameter Combination:\n",
      "==================================================\n",
      "Parameters: {'max_depth': 5, 'min_samples_split': 2}, Validation Accuracy: 0.6526\n",
      "Parameters: {'max_depth': 5, 'min_samples_split': 10}, Validation Accuracy: 0.6509\n",
      "Parameters: {'max_depth': 5, 'min_samples_split': 20}, Validation Accuracy: 0.6491\n",
      "Parameters: {'max_depth': 10, 'min_samples_split': 2}, Validation Accuracy: 0.6596\n",
      "Parameters: {'max_depth': 10, 'min_samples_split': 10}, Validation Accuracy: 0.6456\n",
      "Parameters: {'max_depth': 10, 'min_samples_split': 20}, Validation Accuracy: 0.6474\n",
      "Parameters: {'max_depth': None, 'min_samples_split': 2}, Validation Accuracy: 0.6509\n",
      "Parameters: {'max_depth': None, 'min_samples_split': 10}, Validation Accuracy: 0.6351\n",
      "Parameters: {'max_depth': None, 'min_samples_split': 20}, Validation Accuracy: 0.6246\n",
      "\n",
      "Best Hyperparameters:\n",
      "  {'max_depth': 10, 'min_samples_split': 2}\n",
      "Validation Accuracy with Best Hyperparameters: 0.6596\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters - need to use cross validation and compute validation accuracy\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 10, 20]\n",
    "}\n",
    "\n",
    "# Hyperparamter tuning in order to get the best hyperparameters - perform a 3-fold cross-validation\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy', return_train_score=True)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Extract and display the validation accuracy for each combination\n",
    "print(\"Validation Accuracies for Each Hyperparameter Combination:\")\n",
    "print(\"=\" * 50)\n",
    "cv_results = grid_search.cv_results_\n",
    "for mean_val_acc, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "    print(f\"Parameters: {params}, Validation Accuracy: {mean_val_acc:.4f}\")\n",
    "\n",
    "# Best hyperparameters based on validation accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_validation_accuracy = grid_search.best_score_\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"  {best_params}\")\n",
    "print(f\"Validation Accuracy with Best Hyperparameters: {best_validation_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29afa970-6456-447a-8388-20cd1ebc5cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 80/20 Split:\n",
      "Train Accuracy: 0.9123\n",
      "Test Accuracy: 0.6257\n",
      "--------------------------------------------------\n",
      "Results for 50/50 Split:\n",
      "Train Accuracy: 0.9439\n",
      "Test Accuracy: 0.6199\n",
      "--------------------------------------------------\n",
      "Results for 20/80 Split:\n",
      "Train Accuracy: 0.9825\n",
      "Test Accuracy: 0.6608\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Partition splits with 80/20, 50/50, 20/80\n",
    "splits = [(.8, .2), (.5, .5), (.2, .8)]\n",
    "results = {}\n",
    "split_results = []  # Store split-wise metrics\n",
    "\n",
    "# Outer loop for each partition\n",
    "for split in splits:\n",
    "    train_size, test_size = split\n",
    "    trial_accuracies = {'train': [], 'test': []}\n",
    "    trial_precisions = []\n",
    "    trial_recalls = []\n",
    "    trial_f1_scores = []\n",
    "\n",
    "    # Inner loop to run 3 trials per split\n",
    "    for trial in range(3): \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=None)\n",
    "\n",
    "        # Retrain the model with the best hyperparameters\n",
    "        best_clf = DecisionTreeClassifier(\n",
    "            max_depth=best_params['max_depth'], \n",
    "            min_samples_split=best_params['min_samples_split'], \n",
    "            random_state=42\n",
    "        )\n",
    "        best_clf.fit(X_train, y_train)\n",
    "\n",
    "        # Compute train accuracy\n",
    "        y_train_pred = best_clf.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        trial_accuracies['train'].append(train_accuracy)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        y_test_pred = best_clf.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        trial_accuracies['test'].append(test_accuracy)\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        trial_precisions.append(precision_score(y_test, y_test_pred, average=\"weighted\", zero_division=0))\n",
    "        trial_recalls.append(recall_score(y_test, y_test_pred, average=\"weighted\"))\n",
    "        trial_f1_scores.append(f1_score(y_test, y_test_pred, average=\"weighted\"))\n",
    "\n",
    "    # Average metrics over the 3 trials for the current split\n",
    "    avg_train_accuracy = np.mean(trial_accuracies['train'])\n",
    "    avg_test_accuracy = np.mean(trial_accuracies['test'])\n",
    "    avg_precision = np.mean(trial_precisions)\n",
    "    avg_recall = np.mean(trial_recalls)\n",
    "    avg_f1_score = np.mean(trial_f1_scores)\n",
    "\n",
    "    # Save split-level metrics\n",
    "    split_results.append({\n",
    "        'Split': f\"{int(train_size * 100)}/{int(test_size * 100)}\",\n",
    "        'Test Accuracy': avg_test_accuracy,\n",
    "        'Precision': avg_precision,\n",
    "        'Recall': avg_recall,\n",
    "        'F1-Score': avg_f1_score\n",
    "    })\n",
    "\n",
    "    # Print split-specific results\n",
    "    print(f\"Results for {int(train_size * 100)}/{int(test_size * 100)} Split:\")\n",
    "    print(f\"Train Accuracy: {avg_train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {avg_test_accuracy:.4f}\")\n",
    "    # print(f\"Precision: {avg_precision:.4f}\")\n",
    "    # print(f\"Recall: {avg_recall:.4f}\")\n",
    "    # print(f\"F1-Score: {avg_f1_score:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# # Save the metrics to use later for the summary table\n",
    "# print(\"Overall Metrics for Decision Tree Classifier (Autism Dataset):\")\n",
    "# print(f\"Overall Accuracy: {overall_accuracy_1:.4f}\")\n",
    "# print(f\"Overall Precision: {overall_precision_1:.4f}\")\n",
    "# print(f\"Overall Recall: {overall_recall_1:.4f}\")\n",
    "# print(f\"Overall F1-Score: {overall_f1_score_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d005c81-d439-4e6a-be6b-3aa8697a35d9",
   "metadata": {},
   "source": [
    "#### Results for Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4d85339-2a6a-4fbe-ba83-737320bb729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Results on Liver Dataset:\n",
      "Split     ACC            Precision      Recall         F1-Score       \n",
      "-----------------------------------------------------------------\n",
      "80/20     0.6257         0.6534         0.6257         0.6372         \n",
      "50/50     0.6199         0.6127         0.6199         0.6154         \n",
      "20/80     0.6608         0.6658         0.6608         0.6621         \n",
      "-----------------------------------------------------------------\n",
      "Overall   0.6355         0.6440         0.6355         0.6382         \n"
     ]
    }
   ],
   "source": [
    "# Calculate overall metrics for the Decision Tree classifier\n",
    "overall_accuracy_1 = np.mean([r['Test Accuracy'] for r in split_results])\n",
    "overall_precision_1 = np.mean([r['Precision'] for r in split_results])\n",
    "overall_recall_1 = np.mean([r['Recall'] for r in split_results])\n",
    "overall_f1_score_1 = np.mean([r['F1-Score'] for r in split_results])\n",
    "# Create a formatted table for the split results\n",
    "print(\"Decision Tree Classifier Results on Liver Dataset:\")\n",
    "print(f\"{'Split':<10}{'ACC':<15}{'Precision':<15}{'Recall':<15}{'F1-Score':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Loop through the split results and display each row\n",
    "for result in split_results:\n",
    "    print(f\"{result['Split']:<10}{result['Test Accuracy']:<15.4f}{result['Precision']:<15.4f}{result['Recall']:<15.4f}{result['F1-Score']:<15.4f}\")\n",
    "\n",
    "# Print the overall metrics as the last row\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Overall':<10}{overall_accuracy_1:<15.4f}{overall_precision_1:<15.4f}{overall_recall_1:<15.4f}{overall_f1_score_1:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd96e1-3eb4-44f5-a247-1a9fd4bd065f",
   "metadata": {},
   "source": [
    "### Classifier #2: SVM (with RBF kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eca617cd-0828-423d-90af-bcc15d3d2169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracies for Each Hyperparameter Combination:\n",
      "==================================================\n",
      "Parameters: {'C': 1, 'gamma': 1e-06, 'kernel': 'rbf'}, Validation Accuracy: 0.7123\n",
      "Parameters: {'C': 1, 'gamma': 1e-05, 'kernel': 'rbf'}, Validation Accuracy: 0.7123\n",
      "Parameters: {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}, Validation Accuracy: 0.7123\n",
      "Parameters: {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}, Validation Accuracy: 0.7140\n",
      "Parameters: {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}, Validation Accuracy: 0.7105\n",
      "Parameters: {'C': 10, 'gamma': 1e-06, 'kernel': 'rbf'}, Validation Accuracy: 0.7123\n",
      "Parameters: {'C': 10, 'gamma': 1e-05, 'kernel': 'rbf'}, Validation Accuracy: 0.7053\n",
      "Parameters: {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}, Validation Accuracy: 0.6947\n",
      "Parameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}, Validation Accuracy: 0.6754\n",
      "Parameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}, Validation Accuracy: 0.6930\n",
      "Parameters: {'C': 100, 'gamma': 1e-06, 'kernel': 'rbf'}, Validation Accuracy: 0.7123\n",
      "Parameters: {'C': 100, 'gamma': 1e-05, 'kernel': 'rbf'}, Validation Accuracy: 0.6982\n",
      "Parameters: {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}, Validation Accuracy: 0.6842\n",
      "Parameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}, Validation Accuracy: 0.6667\n",
      "Parameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}, Validation Accuracy: 0.6947\n",
      "Parameters: {'C': 1000, 'gamma': 1e-06, 'kernel': 'rbf'}, Validation Accuracy: 0.7123\n",
      "Parameters: {'C': 1000, 'gamma': 1e-05, 'kernel': 'rbf'}, Validation Accuracy: 0.6789\n",
      "Parameters: {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}, Validation Accuracy: 0.6632\n",
      "Parameters: {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}, Validation Accuracy: 0.6789\n",
      "Parameters: {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}, Validation Accuracy: 0.6947\n",
      "Parameters: {'C': 10000, 'gamma': 1e-06, 'kernel': 'rbf'}, Validation Accuracy: 0.6965\n",
      "Parameters: {'C': 10000, 'gamma': 1e-05, 'kernel': 'rbf'}, Validation Accuracy: 0.6877\n",
      "Parameters: {'C': 10000, 'gamma': 0.0001, 'kernel': 'rbf'}, Validation Accuracy: 0.6404\n",
      "Parameters: {'C': 10000, 'gamma': 0.001, 'kernel': 'rbf'}, Validation Accuracy: 0.6719\n",
      "Parameters: {'C': 10000, 'gamma': 0.01, 'kernel': 'rbf'}, Validation Accuracy: 0.6947\n",
      "\n",
      "Best Hyperparameters:\n",
      "  {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Validation Accuracy with Best Hyperparameters: 0.7140\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100, 1000, 10000],\n",
    "    'gamma': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Set up the SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Perform GridSearchCV on the entire dataset (no need for X_train, Y_train if you're not splitting beforehand)\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=3, scoring='accuracy', return_train_score=True)  # 3-fold cross-validation\n",
    "\n",
    "# Fit the model (this performs the search)\n",
    "grid_search.fit(X, y)  # Use the entire dataset here, not just the training set\n",
    "\n",
    "# Extract the results from the grid search\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "# Print the validation accuracies for each hyperparameter combination\n",
    "print(\"Validation Accuracies for Each Hyperparameter Combination:\")\n",
    "print(\"=\" * 50)\n",
    "for mean_val_acc, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "    print(f\"Parameters: {params}, Validation Accuracy: {mean_val_acc:.4f}\")\n",
    "\n",
    "# Best hyperparameters based on validation accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_validation_accuracy = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding validation accuracy\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"  {best_params}\")\n",
    "print(f\"Validation Accuracy with Best Hyperparameters: {best_validation_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "982c152a-9229-4876-a97e-62f6c153b0c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for 80/20 Split:\n",
      "Train Accuracy: 0.7763\n",
      "Test Accuracy: 0.6812\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for 50/50 Split:\n",
      "Train Accuracy: 0.8012\n",
      "Test Accuracy: 0.7110\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for 20/80 Split:\n",
      "Train Accuracy: 0.8158\n",
      "Test Accuracy: 0.6968\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters obtained from GridSearchCV\n",
    "best_C = 1\n",
    "best_gamma = 0.001\n",
    "\n",
    "# Split ratios\n",
    "split_ratios = [(0.8, 0.2), (0.5, 0.5), (0.2, 0.8)]\n",
    "split_results_svm_2 = []\n",
    "\n",
    "# Iterate over splits\n",
    "for train_size, test_size in split_ratios:\n",
    "    trial_train_accuracies = []\n",
    "    trial_validation_accuracies = []\n",
    "    trial_test_accuracies = []\n",
    "    trial_precisions = []\n",
    "    trial_recalls = []\n",
    "    trial_f1_scores = []\n",
    "    \n",
    "    for _ in range(3):  # Perform 3 trials for each split\n",
    "        # Split the data into train and temp (validation + test)\n",
    "        X_train, X_temp, Y_train, Y_temp = train_test_split(X, y, train_size=train_size, stratify=y, random_state=None)\n",
    "        \n",
    "        # Now split the temp data into validation and test\n",
    "        X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=test_size, stratify=Y_temp, random_state=None)\n",
    "\n",
    "        # Train the SVM model using the best hyperparameters\n",
    "        clf = SVC(C=best_C, gamma=best_gamma, kernel='rbf')\n",
    "        clf.fit(X_train, Y_train)\n",
    "\n",
    "        # Calculate train accuracy\n",
    "        train_accuracy = clf.score(X_train, Y_train)\n",
    "        trial_train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        validation_accuracy = clf.score(X_val, Y_val)\n",
    "        trial_validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        test_accuracy = clf.score(X_test, Y_test)\n",
    "        trial_test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        Y_test_pred = clf.predict(X_test)\n",
    "        trial_precisions.append(precision_score(Y_test, Y_test_pred, average=\"weighted\", zero_division=0))\n",
    "        trial_recalls.append(recall_score(Y_test, Y_test_pred, average=\"weighted\"))\n",
    "        trial_f1_scores.append(f1_score(Y_test, Y_test_pred, average=\"weighted\"))\n",
    "\n",
    "    # Calculate and store split-wise average metrics\n",
    "    avg_train_accuracy = np.mean(trial_train_accuracies)\n",
    "    avg_validation_accuracy = np.mean(trial_validation_accuracies)\n",
    "    avg_test_accuracy = np.mean(trial_test_accuracies)\n",
    "    avg_precision = np.mean(trial_precisions)\n",
    "    avg_recall = np.mean(trial_recalls)\n",
    "    avg_f1_score = np.mean(trial_f1_scores)\n",
    "\n",
    "    split_results_svm_2.append({\n",
    "        'Split': f\"{int(train_size * 100)}/{int(test_size * 100)}\",\n",
    "        'ACC': avg_test_accuracy,\n",
    "        'Precision': avg_precision,\n",
    "        'Recall': avg_recall,\n",
    "        'F1-Score': avg_f1_score\n",
    "    })\n",
    "\n",
    "    # Print split-specific results\n",
    "    print(f\"\\nResults for {int(train_size * 100)}/{int(test_size * 100)} Split:\")\n",
    "    print(f\"Train Accuracy: {avg_train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {avg_test_accuracy:.4f}\")\n",
    "    # print(f\"Precision: {avg_precision:.4f}\")\n",
    "    # print(f\"Recall: {avg_recall:.4f}\")\n",
    "    # print(f\"F1-Score: {avg_f1_score:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff3c41-cf1d-4383-8ff4-1b0483a9d6da",
   "metadata": {},
   "source": [
    "#### Results for SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c74742af-88af-4af4-918d-ce1991adaca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Results on Liver Dataset:\n",
      "Split     ACC            Precision      Recall         F1-Score       \n",
      "---------------------------------------------------------------------------\n",
      "80/20     0.6812         0.4807         0.6812         0.5636         \n",
      "50/50     0.7110         0.6728         0.7110         0.6527         \n",
      "20/80     0.6968         0.6095         0.6968         0.6129         \n",
      "---------------------------------------------------------------------------\n",
      "Overall   0.6963         0.5877         0.6963         0.6097         \n"
     ]
    }
   ],
   "source": [
    "# Calculate overall metrics for the SVM classifier\n",
    "overall_accuracy_2 = np.mean([r['ACC'] for r in split_results_svm_2])\n",
    "overall_precision_2 = np.mean([r['Precision'] for r in split_results_svm_2])\n",
    "overall_recall_2 = np.mean([r['Recall'] for r in split_results_svm_2])\n",
    "overall_f1_score_2 = np.mean([r['F1-Score'] for r in split_results_svm_2])\n",
    "\n",
    "# Create a formatted table for the split results\n",
    "print(\"SVM Classifier Results on Liver Dataset:\")\n",
    "print(f\"{'Split':<10}{'ACC':<15}{'Precision':<15}{'Recall':<15}{'F1-Score':<15}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Loop through the split results and display each row\n",
    "for result in split_results_svm_2:\n",
    "    print(f\"{result['Split']:<10}{result['ACC']:<15.4f}{result['Precision']:<15.4f}{result['Recall']:<15.4f}{result['F1-Score']:<15.4f}\")\n",
    "\n",
    "# Print the overall metrics as the last row\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Overall':<10}{overall_accuracy_2:<15.4f}{overall_precision_2:<15.4f}{overall_recall_2:<15.4f}{overall_f1_score_2:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b571801-40a4-4a4f-8317-37ea4692a9be",
   "metadata": {},
   "source": [
    "### Classifier #3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8eb3d39e-6c13-4fdc-b257-c10532007a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracies for Each Hyperparameter Combination:\n",
      "==================================================\n",
      "Parameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}, Validation Accuracy: 0.7107\n",
      "Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}, Validation Accuracy: 0.7020\n",
      "Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}, Validation Accuracy: 0.6933\n",
      "Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}, Validation Accuracy: 0.6759\n",
      "Parameters: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}, Validation Accuracy: 0.6846\n",
      "\n",
      "Best Hyperparameters:\n",
      "  {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Validation Accuracy with Best Hyperparameters: 0.7107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `X_train`, `y_train` are already defined.\n",
    "\n",
    "# Hyperparameter tuning for Logistic Regression\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l2'], 'solver': ['lbfgs']}\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results from the grid search\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "# Print the validation accuracies for each hyperparameter combination\n",
    "print(\"Validation Accuracies for Each Hyperparameter Combination:\")\n",
    "print(\"=\" * 50)\n",
    "for mean_val_acc, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "    print(f\"Parameters: {params}, Validation Accuracy: {mean_val_acc:.4f}\")\n",
    "\n",
    "# Best hyperparameters based on validation accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_validation_accuracy = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding validation accuracy\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"  {best_params}\")\n",
    "print(f\"Validation Accuracy with Best Hyperparameters: {best_validation_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f00c488-01ac-4931-a35c-48efd7443b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for 80/20 Split:\n",
      "Avg Train Accuracy = 0.7376\n",
      "Avg Test Accuracy = 0.6377\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for 50/50 Split:\n",
      "Avg Train Accuracy = 0.7392\n",
      "Avg Test Accuracy = 0.6993\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for 20/80 Split:\n",
      "Avg Train Accuracy = 0.7661\n",
      "Avg Test Accuracy = 0.7123\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_C = 1\n",
    "best_penalty = 'l2'\n",
    "best_solver = 'lbfgs'\n",
    "\n",
    "# Assuming `X` and `y` are your feature and target arrays\n",
    "# Split ratios\n",
    "split_ratios = [(0.8, 0.2), (0.5, 0.5), (0.2, 0.8)]\n",
    "\n",
    "# Lists for storing accuracies and other metrics\n",
    "split_results_logreg = []\n",
    "\n",
    "# Iterate over splits\n",
    "for train_size, test_size in split_ratios:\n",
    "    trial_train_accuracies = []\n",
    "    trial_validation_accuracies = []\n",
    "    trial_test_accuracies = []\n",
    "    trial_precisions = []\n",
    "    trial_recalls = []\n",
    "    trial_f1_scores = []\n",
    "    \n",
    "    for _ in range(3):  # Running 3 trials\n",
    "        # Split the data into train and temp (validation + test)\n",
    "        X_train, X_temp, Y_train, Y_temp = train_test_split(X, y, train_size=train_size, random_state=None)\n",
    "        \n",
    "        # Now split the temp data into validation and test\n",
    "        X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=test_size, random_state=None)\n",
    "\n",
    "        # Train the Logistic Regression model using the best hyperparameters\n",
    "        clf = LogisticRegression(C=best_C, penalty=best_penalty, solver=best_solver, max_iter=10000)\n",
    "        clf.fit(X_train, Y_train)\n",
    "\n",
    "        # Calculate the accuracy on the training set\n",
    "        train_accuracy = clf.score(X_train, Y_train)\n",
    "        trial_train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Calculate the accuracy on the validation set\n",
    "        validation_accuracy = clf.score(X_val, Y_val)\n",
    "        trial_validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "        # Calculate the accuracy on the test set\n",
    "        test_accuracy = clf.score(X_test, Y_test)\n",
    "        trial_test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Calculate precision, recall, and F1-score on the test set\n",
    "        Y_test_pred = clf.predict(X_test)\n",
    "        precision = precision_score(Y_test, Y_test_pred, average=\"weighted\", zero_division=0)\n",
    "        recall = recall_score(Y_test, Y_test_pred, average=\"weighted\")\n",
    "        f1 = f1_score(Y_test, Y_test_pred, average=\"weighted\")\n",
    "        trial_precisions.append(precision)\n",
    "        trial_recalls.append(recall)\n",
    "        trial_f1_scores.append(f1)\n",
    "\n",
    "    # Calculate and store average metrics for the current split\n",
    "    avg_train_accuracy = np.mean(trial_train_accuracies)\n",
    "    avg_validation_accuracy = np.mean(trial_validation_accuracies)\n",
    "    avg_test_accuracy = np.mean(trial_test_accuracies)\n",
    "    avg_precision = np.mean(trial_precisions)\n",
    "    avg_recall = np.mean(trial_recalls)\n",
    "    avg_f1_score = np.mean(trial_f1_scores)\n",
    "\n",
    "    split_results_logreg.append({\n",
    "        'Split': f\"{int(train_size * 100)}/{int(test_size * 100)}\",\n",
    "        'Test Accuracy': avg_test_accuracy,\n",
    "        'Precision': avg_precision,\n",
    "        'Recall': avg_recall,\n",
    "        'F1-Score': avg_f1_score\n",
    "    })\n",
    "\n",
    "    # Print split-specific results\n",
    "    print(f\"\\nResults for {int(train_size * 100)}/{int(test_size * 100)} Split:\")\n",
    "    print(f\"Avg Train Accuracy = {avg_train_accuracy:.4f}\")\n",
    "    print(f\"Avg Test Accuracy = {avg_test_accuracy:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60cfb5-a50b-4464-837a-e8f2c9454ad2",
   "metadata": {},
   "source": [
    "#### Results for Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2751fa35-71a9-45b0-8883-2b8b42493300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Classifier Results on Liver Dataset:\n",
      "Split     ACC            Precision      Recall         F1-Score       \n",
      "----------------------------------------------------------------------\n",
      "80/20     0.6377         0.6525         0.6377         0.5880         \n",
      "50/50     0.6993         0.6685         0.6993         0.6721         \n",
      "20/80     0.7123         0.6878         0.7123         0.6923         \n",
      "----------------------------------------------------------------------\n",
      "Overall   0.6831         0.6696         0.6831         0.6508         \n"
     ]
    }
   ],
   "source": [
    "# Calculate overall metrics\n",
    "overall_accuracy_3 = np.mean([r['Test Accuracy'] for r in split_results_logreg])\n",
    "overall_precision_3 = np.mean([r['Precision'] for r in split_results_logreg])\n",
    "overall_recall_3 = np.mean([r['Recall'] for r in split_results_logreg])\n",
    "overall_f1_score_3 = np.mean([r['F1-Score'] for r in split_results_logreg])\n",
    "\n",
    "# Create a formatted table for the split results\n",
    "print(\"\\nLogistic Regression Classifier Results on Liver Dataset:\")\n",
    "print(f\"{'Split':<10}{'ACC':<15}{'Precision':<15}{'Recall':<15}{'F1-Score':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Loop through the split results and display each row\n",
    "for result in split_results_logreg:\n",
    "    print(f\"{result['Split']:<10}{result['Test Accuracy']:<15.4f}{result['Precision']:<15.4f}{result['Recall']:<15.4f}{result['F1-Score']:<15.4f}\")\n",
    "\n",
    "# Print the overall metrics as the last row\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Overall':<10}{overall_accuracy_3:<15.4f}{overall_precision_3:<15.4f}{overall_recall_3:<15.4f}{overall_f1_score_3:<15.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d8817-99b3-4f4a-8b9f-1f41386b613f",
   "metadata": {},
   "source": [
    "## Final Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f41b08a-e47a-47ec-b23c-b46446e05492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary Table of Overall Metrics:\n",
      "                Classifier       ACC  Precision    Recall  F1-Score\n",
      "0            Decision Tree  0.635478   0.643981  0.635478  0.638238\n",
      "1  Support Vector Machines  0.696306   0.587669  0.696306  0.609749\n",
      "2      Logistic Regression  0.683104   0.669586  0.683104  0.650806\n"
     ]
    }
   ],
   "source": [
    "summary_data = {\n",
    "    'Classifier': ['Decision Tree', 'Support Vector Machines', 'Logistic Regression'],  # Add names for all classifiers\n",
    "    'ACC': [overall_accuracy_1, overall_accuracy_2, overall_accuracy_3],  # Add more as needed\n",
    "    'Precision': [overall_precision_1, overall_precision_2, overall_precision_3],\n",
    "    'Recall': [overall_recall_1, overall_recall_2, overall_recall_3],\n",
    "    'F1-Score': [overall_f1_score_1, overall_f1_score_2, overall_f1_score_3]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for cleaner display\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Print the summary table\n",
    "print(\"Final Summary Table of Overall Metrics:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6b3da-cdb8-43b1-8ab1-9a2be7092e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
